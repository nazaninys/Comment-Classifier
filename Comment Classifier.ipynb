{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "# پروژه سوم - Naive Bayes Classifier \n",
    "## نازنین یوسفیان\n",
    "### 810197610"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "## هدف\n",
    "### در این پروژه قصد داریم که تعدادی کامنت را تحلیل کرده و مشخص کنیم که با توجه به متن آن، کامنت مثبتی بوده است یا خیر. در این جا از الگوریتم Naive Bayes Classifier استفاده می کنیم.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "## مقدمه\n",
    "### دو فایل به نام های *comment_train* و *comment_test* در اختیار ما قرار گرفته اند. ابتدا باید کامنت های موجود در train را تحلیل کرده و جدول احتمالات هر کلمه به شرط recommended بود و نبودن را به دست آوریم. سپس برای هر کامنت در test تصمیم بگیریم که چه مقداری برای آن مناسب است. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "### توضیح پروژه\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import collections\n",
    "file_name1 = 'comment_train.csv'\n",
    "file_name2 = 'comment_test.csv'\n",
    "data = pd.read_csv(file_name1)\n",
    "test = pd.read_csv(file_name2)\n",
    "data['new'] = data['comment'] + ' ' + data['title']\n",
    "test['new'] = test['comment'] + ' ' + test['title']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "### پیش پردازش داده\n",
    "برای تحلیل درست تر، بهتر است که از پیش پردازش روی داده ها استفاده کنیم. در مرحله اول ستون کامنت و تیتر را با یکدیگر ترکیب کرده و به عنوان یک ستون در نظر می گیریم. ابتدا از تابع normalize استفاده می کنیم که نیم فاصله ها و فاصله ها را در متن به درستی مشخص می کند تا در مراحل بعد از توابع دیگر نتیجه بهتری بگیریم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(x):\n",
    "    normalizer = Normalizer()\n",
    "    return normalizer.normalize(x)\n",
    "    \n",
    "data['new'] = data['new'].apply(normal)\n",
    "test['new'] = test['new'].apply(normal)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "در مرحله بعدی متونی که داریم را به کلمات تشکیل دهنده آن با استفاده از تابع *()word_tokenize* تجزیه می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(x):\n",
    "    return word_tokenize(x)\n",
    "data['new'] = data['new'].apply(token)\n",
    "test['new'] = test['new'].apply(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حال تابع *()lemmatize* را روی کلمات مشخص شده صدا می زنیم. این تابع کمک می کند که اطلاعات اضافه کلمات مانند پیشوندها ، نشانه های جمع و ...حذف شوند. هم چنین برای فعل ها بن ماضی و مضارع آن را به ما بر می گرداند که برای تحلیل مفیدتر هستند. زیرا فرقی نمی کند که از یک فعل در چه زمانی استفاده شده باشد و یا چه شناسه ای داشته باشد. هم چنین در واژه ها نشانه های جمع برای ما تفاوتی ایجاد نمی کنند و بهتر است که یک کلمه و جمع آن از یکدیگر متمایز نباشند. \n",
    "در این جا چون نظرات به زبان محاوره ای نوشته شده اند، در بعضی از موارد نمی تواند کلمات را به درستی به ما برگرداند ولی در موارد دیگر به درستی کار می کند و دقت آن برای ما مطلوب است. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def lemma(x):\n",
    "    for i in x:\n",
    "        lemmatizer.lemmatize(i)\n",
    "    return x\n",
    "\n",
    "data['new'] = data['new'].apply(lemma)\n",
    "test['new'] = test['new'].apply(lemma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "پیش پردازش دیگری که از آن می توانیم استفاده کنیم حذف کلمات پرتکرار است. به طور مثال حروف اضافه در متن ها زیاد به کار می روند ولی تاثیری در تحلیلی که انجام می دهیم ندارند و بر اساس آن ها نمی توان تصمیم گرفت که آن نظر مثبت محسوب می شود یا خیر. در  *stopwords_list* لیستی از این کلمات وجود دارد و ما آن ها را از نظرات پاک کرده ایم. کلماتی مانند *عالی* و *بسیار* نیز در این لیست وجود دارند که حذف نکردن آن ها می تواند برای تحلیل بهتر باشد ولی با حذف آن ها نیز تحلیل ما از دقت قابل قبولی برخوردار است. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords_list())\n",
    "def del_stopword(x):\n",
    "    new_set = x.copy()\n",
    "    for i in x:\n",
    "        if i in stopwords:\n",
    "             new_set.remove(i)\n",
    "    return new_set\n",
    "            \n",
    "data['new'] = data['new'].apply(del_stopword)\n",
    "test['new'] = test['new'].apply(del_stopword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "### فرآیند حل مسئله\n",
    "برای حل این مسئله از مدل bag of words استفاده می کنیم به این صورت که هر کلمه را مستقل از جایگاهش در نظر می گیریم. هر کلمه و اینکه آن کامنت پیشنهاد شده است یا نه یک feature محسوب می شود. در Naive Bayes پیشنهاد شدن یا نشدن را پدر همه feature های دیگر در نظر می گیریم و این فرض را داریم که فرزندانش به شرط دانستن پدر از یکدیگر مستقل هستند که فرض قوی ای است و در واقعیت این گونه نیست. در یک متن کلمات به یکدیگر وابسته اند ولی با این فرض هم نتیجه ای که به دست می آید از دقت بالایی برخوردار است. پس اگر اینکه کامنتی مثبت باشد را c در نظر بگیریم، می خواهیم P(c|X) را حساب کنیم که X کلماتی هستند که در آن کامنت وجود دارند.\n",
    "پس در کامنت جدیدی که به دست ما می رسد، می خواهیم بدانیم با توجه به کلماتی که در آن به کار رفته است کامنت مثبتی است یا خیر. برای به دست آوردن این احتمال ابتدا باید احتمال p(c,X) را حساب کنیم که چون x ها به شرط دانستن c از یکدیگر مستقل اند این احتمال به صورت زیر ساده می شود:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(c | X) = p(c) * p(x1 | c) * p(x2 | c) *…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "در اینجا posterior probability احتمال مثبت یا منفی بودن کامنت در فایل test به شرط کلماتی است که در آن به کار رفته است. Likelihood احتمال وجودهر کلمه در کامنت به شرط مثبت یا منفی بودن کامنت است که از تحلیل داده های فایل train به دست می آوریم. Class Prior Probability احتمال مثبت بودن یا منفی بودن یک کامنت است که برای مثبت ها تعداد کل recommended ها را در فایل train به کل کامنت ها تقسیم می کنیم ت احتمال آن بدست بیاید. به همین صورت احتمال منفی بودن به دست می آید. Predictor Prior Probability احتمال وجود یک کلمه در کل کامنت هاست.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "با استفاده از  فایل *comment_train* باید احتمالاتی که در سمت راست تساوی آمده اند را محاسبه کنیم. ابتدا باید در سطرهایی که recommended هستند کلمات متمایز به کار رفته را جدا کنیم و به ازای هر کدام حساب کنیم که چند بار تکرار شده اند. هم چنین باید این محاسبات را برای سطرهای not_recommended تکرار کنیم. این گونه likelihood را محاسبه می کنیم. سپس باید ببینیم که به ازای همه کامنت ها چند درصدشان مثبت و چند درصد منفی بوده اند و بر اساس آن احتمال class prior probability را حساب کنیم.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom_words = []\n",
    "notrecom_words = []\n",
    "recom = len(data[data['recommend'] == 'recommended'])\n",
    "not_recom = len(data[data['recommend'] == 'not_recommended'])\n",
    "total = recom + not_recom\n",
    "def add_recom_words(x):\n",
    "    for item in x:\n",
    "        recom_words.append(item)\n",
    "def add_notrecom_words(x):\n",
    "    for item in x:\n",
    "        notrecom_words.append(item)\n",
    "\n",
    "\n",
    "data['new'][data['recommend'] == 'recommended'].apply(add_recom_words)\n",
    "data['new'][data['recommend'] == 'not_recommended'].apply(add_notrecom_words)\n",
    "recom_table = collections.Counter(recom_words)\n",
    "notrecom_table = collections.Counter(notrecom_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "###  Additive Smoothing \n",
    "اگر در داده های train کلمه ای در دسته recommended باشد و در دسته not_recommended نباشد، احتمال p(x | recommended) محاسبه نشده و صفر در نظر گرفته می شود. حال اگر به کامنتی در فایل test برخورد کنیم که این کلمه در آن استفاده شده است چون احتمال p(x | recommended) صفر در نظر گرفته شده است باعث می شود احتمال recommended بود آن کامنت صفر شود و not_recommended در نظر گرفته شود. اینگونه به بقیه کلماتی که در آن کامنت وجود داشته اند توجهی نمی شود و صرفا بر اساس یک کلمه تصمیم گیری می شود. همین اتفاق برای وقتی که کلمه در هیچ کدام از دسته ها نباشد نیز می افتد. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "برای حل این مشکل از روش Additive Smoothing استفاده می شود. در این روش یک hyper parameter به نام α در نظر گرفته می شود. عدد α به صورت احتمالات و عدد α * k به مخرج آن ها اضافه می شوند و احتمالات به صورت زیر محاسبه می شوند:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(x | c) = ( p(x,c) + α ) / ( p(c) + k * α )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "که در اینجا k تعداد کلمه هایی است که در دسته c قرار دارند. دلیل اضافه کردن k * α به جای α در مخرج این است که چون این کلمه، کلمه جدیدی است و هیج پیش زمینه ای راجع به آن نداریم پس احتمال آن را به طور یکنواخت روی k دسته بندی حساب می کنیم. با این روش دیگر به احتمال 0 در محاسبات برخورد نمی کنیم  و باعث می شود که بتوانیم بر اساس کلمات دیگری که در کامنت وجود دارند و احتمالشان را می دانیم تصمیم گیری کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.93625\n",
      "Precision:  0.9204819277108434\n",
      "Recall:  0.955\n",
      "F1 0.9374233128834355\n"
     ]
    }
   ],
   "source": [
    "def predict(x):\n",
    "    recom_prob = math.log(recom / total)\n",
    "    notrecom_prob = math.log(not_recom / total)\n",
    "    alpha = 0.05\n",
    "    for word in x:\n",
    "        if word in recom_table:\n",
    "            recom_prob += math.log(recom_table[word] / recom)\n",
    "        else:\n",
    "            recom_prob += math.log(alpha / (recom + alpha * total))\n",
    "            \n",
    "            \n",
    "        if word in notrecom_table:\n",
    "            notrecom_prob += math.log(notrecom_table[word] / not_recom)\n",
    "        else:\n",
    "            notrecom_prob += math.log(alpha / (not_recom + alpha * total))\n",
    "           \n",
    "    if recom_prob > notrecom_prob:\n",
    "        return 'recommended'\n",
    "    else:\n",
    "         return 'not_recommended'\n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "test['predict'] = test['new'].apply(predict)\n",
    "accuracy = len(test[test['recommend'] == test['predict']]) / len(test)\n",
    "print('Accuracy: ', accuracy)\n",
    "precision = len(test[(test['recommend'] == test['predict']) & (test['predict'] == 'recommended')]) / len(test[test['predict'] == 'recommended'])\n",
    "print('Precision: ', precision)\n",
    "recall =  len(test[(test['recommend'] == test['predict']) & (test['predict'] == 'recommended')]) / len(test[test['recommend'] == 'recommended'])\n",
    "print('Recall: ', recall)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('F1', f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "precision نسبت تعداد کامنت هایی که درست recommended تشخیص داده ایم را به کل کامنت هایی که recommended تشخیص داده ایم بیان می کند در حالی که recall نسبت تعداد کامنت هایی که درست recommended تشخیص داده ایم را به کل کامنت هایی که در واقعیت recommended بوده اند بیان می کند. در حالت هایی که اشتباه مثبت تشخیص دادن ( یعنی حالتی که در واقعیت منفی بوده و ما به اشتباه مثبت تشخیص می دهیم) هزینه زیادی برای ما دارد مثلا در تشخیص spam بودن یک ایمیل، precision برای ما مهم است و باید درصد بالایی داشته باشد. در حالت هایی که اشتباه منفی تشخیص دادن برای ما هزینه زیادی دارد recall اهمیت بالایی دارد. به همین دلیل باید هر دوی این ویژگی ها را حساب کنیم و بالا بودن درصد یکی لزوما بالا بودن درصد دیگری را به همراه ندارد و بین این دو یک trade-off برقرار است. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حالتی را در نظر بگیریم که تعداد کمی کامنت را recommended تشخیص داده ایم(مثلا 100 تا) و از این بین تعداد زیادی را به درستی تشخیص داده ایم (مثلا 90 تا). با اینکه معیار precision در این حالت خیلی بالا است ولی recall خیلی کم است و مدل ما خوب کار نمی کند. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حالت دیگری که ممکن است پیش بیاید و مورد قبول ما نباشد این است که از بین کامنت هایی که در واقیت recommended هستند، تعداد زیادی شان را به درستی تشخیص بدهیم ولی تعداد زیادی از not_recommended ها را نیز recommended تشخیص بدهیم که در اینجا معیار recall مقدار بالایی دارد در صورتی که precision دقت کافی را ندارد."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "برای حل مشکلاتی که در بالا ذکر شد، به طور کلی می توانیم از معیار F1 استفاده کنیم که میانگین هارمونیک دو معیار precision و recall است. به جای اینکه به دنبال حالتی باشیم که هم precision و هم recall درصد خوبی داشته باشند، می توانیم حالتی را انتخاب کنیم که F1 بالایی داشته باشد. در واقع F1 یک تعادل بین این دو معیار برقرار می کند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "در مواردی که درست تشخیص دادن مثبت و منفی بودن برای ما مهم تر است، از معیار accuracy استفاده می کنیم و در حالاتی که می خواهیم تشخیص اشتباهمان کمتر باشد، به معیار F1 بیشتر توجه می کنیم. هم چنین اگر توزیع ویژگی یکسان نیست مثلا تعداد recommended ها از not_recommended ها خیلی بیشتر است، F1 ما را به مدل بهتری می رساند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "#### جدول معیار ها در حالات متفاوت "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Accuracy | Precision | Recall | F1|\n",
    "|--|----------|-----------|--------|---|\n",
    "|a|0.93|0.90|0.96|0.93|\n",
    "|b|0.92|0.88|0.98|0.93|\n",
    "|c|0.91|0.90|0.92|0.91|\n",
    "|d|0.90|0.87|0.94|0.91|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "تاثیر Additive Smoothing در تحلیل درست داده بیشتر از پیش پردازش است. زیرا ممکن است که ما داده ها را به خوبی پیش پردازش کرده و کلمات را به خوبی جایگزین کنیم ولی هنگامی که با کلمه جدیدی برخورد می کنیم که در لیست احتمالاتمان نیست، تمام پردازش هایی که روی بقیه کلمات انجام داده ایم بی تاثیر شده و احتمال نهایی صفر می شود که درصد خطا را بالا می برد. انجام پیش پردازش خطای ما را کمتر می کند و روش هایی که استفاده می کنیم در نتیجه نهایی موثر است. باید دقت شود که در تحلیل داده ها، نوع داده ها از چه دسته ای هستند و بر اساس آن کلمات پرتکرار را انتخاب کرده و حذف کنیم. ممکن است کلمه پرتکراری داشته باشیم که در تحلیل نوعی از داده موثر است و نباید حذف شود و حذف نکردن آن دقت را بالا ببرد در صورتی که در نوع دیگری از داده حذف نکردن این کلمه باعث خطای بیشتری شود.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "هم چنین مشاهده می شود که مقدار Recall نسبت به Precision بالاتر است و دقت بیشتری دارد. درصد بالایی از کامنت هایی که recommended بوده اند را به درستی تشخیص داده ایم ولی کامنت هایی که recommended تشخیص داده ایم، بعضی هایشان not_recommended بوده اند و درصد خطایمان در این حالت بیشتر بوده است. مقادیر F1 و Accuracy تقریبا با یکدیگر برابرند و این به این علت است که توزیع داده تقریبا یکسان بوده است."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "#### کامنت هایی که اشتباه تشخیص داده شده اند:\n",
    "1. بعد از ۱ سال استفاده  بعد از ۱ سال استفاده کاملا چسب هاش باز شده و شکلی زشت پیدا کرده مجبور شدم درش بیارم، برش هاشم دقیق نیست خیلی بده تنها خوبیش این بود راحت در آوردم و انداختم تو سطل آشغال\n",
    "2. عالی است عالی عالی است\n",
    "3. دیر شارژ میکند اصلا کیفیت ندارد. 20 ساعت طول میکشه تا گوشی رو شارژ کنه\n",
    "4. روکش بالشتاش اصلا شبیه عکسش نبود صورتی ساده بود روتختیش نسبت به بقیه روتختیا کوچیکتره جوری که از تخت آویز نمیشه لب به لب تخت اندازست ایکاش اون عکسی که میزارن با اونی که میفرستن یکی بود\n",
    "5. خیالم راحت شد فندک قبلیم مدام فیوز میسوزوند و یک بار شارژر موبایل هم سوزوند ولی با این هیچ مشکلی بوجود نیومده تا الان. کیفیتش خیلی خوبه و لامپ هم داره "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "کامنت های 1، 3و 4 به درستی لیبل نخورده اند. یعنی از متن کامنت اینگونه برداشت می شود که باید not_recommended باشند در صورتی که در فایل test لیبل recommended خورده اند و درواقع ما به درستی آن ها را تشخیص داده ایم. \n",
    "در مورد دوم *عالی* و *است* جزو کلمات پرتکراری هستند که حذف شده اند و به همین علت نتوانسته ایم به درستی تشخیصشان بدهیم. \n",
    "مورد پنجم کلماتی دارد که احتمال اینکه به آن ها در train برخورده باشیم کم است و در واقع پیش زمینه ای برای آن ها نداریم. هم چنین از جمله دوم و سوم به تنهایی ممکن است این برداشت شود که از این محصول رضایت ندارد. هم چنین کلمه *مشکل* احتمال زیادی دارد که در دسته not_recommended باشد."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "از مشکلاتی که تحلیل ما دارد این است که هر کلمه را مستقل در نظر می گیریم در صورتی که می دانیم کلمات در یک جمله به یکدیگر وابسته اند و باید به مفهوم جمله توجه شود. هم چنین علائم نگارشی در تحلیل تاثیر زیادی دارند. ممکن است در نظری که recommended است ابتدا ایرادات جزئی محصول گفته شود ولی درکل رضایت از آن اعلام شود که این تحلیل ما را با مشکل رو به رو می کند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "## نتیجه گیری\n",
    "### مدل Naive Bayes مدل ساده ای است که بر اساس آن می توان تعدادی داده را دسته بندی کرد. الگوریتم آن در عین سادگی از دقت خوبی برخوردار است. از مشکلات این الگوریتم این است که هر کلمه به شرط دانستن ویژگی که به دنبال یافتن آن هستیم، از بقیه کلمات مستقل است و این ممکن است دقت تحلیل ما را پایین بیاورد. در صورتی که داده هایی که برای train انتخاب می شوند، رندوم باشند و توزیع یکسان داشته باشند این الگوریتم می تواند به خوبی عمل کند. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "## منابع"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                                                                               \n",
    "1. *https://medium.com/analytics-vidhya/intuition-behind-naive-bayes-algorithm-laplace-additive-smoothing-e2cb43a82901*\n",
    "2. *https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf*\n",
    "3. *https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/*\n",
    "4. *https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488*"
   ]
  }
 ],
 "metadata": {
  "direction": "ltr",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
